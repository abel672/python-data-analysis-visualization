{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# for Data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "\n",
    "# Math\n",
    "import math\n",
    "\n",
    "# Plot imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib\n",
    "\n",
    "# Machine Learning Imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For evaluating our ML results\n",
    "from sklearn import metrics\n",
    "\n",
    "# Dataset Import\n",
    "import statsmodels.api as sm"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using matplotlib backend: MacOSX\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 1: Basic Mathematical Overview\n",
    "\n",
    "First, let's take a look at the [Logistic Function](https://en.wikipedia.org/wiki/Logistic_function). The logistic function can take an input from negative to positive infinity and it has always has an output between 0 and 1. The logistic function is defined as:$$ \\sigma (t)= \\frac{1}{1+e^{-t}}$$\n",
    "\n",
    "Here a [tutorial](https://www.youtube.com/watch?v=TPqr8t919YM&ab_channel=PowerH)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Logistic function\n",
    "def logistic(t):\n",
    "    return 1.0 / (1 + math.exp((-1.0)*t) )\n",
    "\n",
    "# Set t from -6 to 6 ( 500 elements, linearly spaced)\n",
    "t = np.linspace(-6,6,500)\n",
    "\n",
    "# Set up y values (using list comprehesion) explanation: https://www.w3schools.com/python/python_lists_comprehension.asp\n",
    "y = np.array([logistic(ele) for ele in t])\n",
    "\n",
    "# Plot\n",
    "plt.plot(t,y)\n",
    "plt.title(' Logistic Function ')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, ' Logistic Function ')"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we remember back to the [Linear Regression](https://en.wikipedia.org/wiki/Linear_regression) Lectures, we could describe a Linear Regression Function model as:$$ y_i = \\beta _1 x_{i1} + ... + \\beta _i x_{ip}$$\n",
    "\n",
    "Which was basically an expanded linear equation (y=mx+b) for various x data features. In the case of the above equation, we presume a data set of 'n' number of units, so that the data set would have the form:$$ [ y_i, x_{i1},...,x_{ip}]^{n}_{i=1}$$\n",
    "\n",
    "For our logistic function, if we view t as a linear function with a variable x we could express t as:$$ t = \\beta _0 + \\beta _1 x $$\n",
    "\n",
    "Here, we've basically just substituted a linear function (form similar to y=mx+b) for t. We could then rewrite our logistic function equation as:$$ F(x)= \\frac{1}{1+e^{-(\\beta _0 + \\beta _1 x)}}$$\n",
    "\n",
    "Now we can interpret F(x) as the probability that the dependent variable is a \"success\" case, this is a similar style of thinking as in the Binomial Distribution, in which we had successes and failures. So the formula for F(x) that we have here states that the probability of the dependent variable equaling a \"success\" case is equal to the value of the logistic function of the linear regression expression (the linear equation we used to replace t ).\n",
    "\n",
    "Inputting the linear regression expression into the logistic function allows us to have a linear regression expression value that can vary from positive to negative infinity, but after the transformation due to the logistic expression we will have an output of F(x) that ranges from 0 to 1.\n",
    "\n",
    "We can now perform a binary classification based on where F(x) lies, either from 0 to 0.5, or 0.5 to 1."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 2: Extra Math Resources\n",
    "\n",
    "This is a very basic overview of binary classification using Logistic Regression, if you're still interested in a deeper dive into the mathematics, check out these sources:\n",
    "\n",
    "1.) [Andrew Ng's class notes](http://cs229.stanford.edu/notes2020spring/cs229-notes1.pdf) on Logistic Regression (Note: Scroll down)\n",
    "\n",
    "2.) [CMU notes Note](http://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch12.pdf): Advanced math notation.\n",
    "\n",
    "3.) [Wikipedia](https://en.wikipedia.org/wiki/Logistic_regression) has a very extensive look at logistic regression.\n",
    "\n",
    "4.) Logistic Regression [entire tutorial](https://www.youtube.com/watch?v=yIYKR4sgzI8&list=PLblh5JKOoLUKxzEP5HA2d-Li7IJkHfXSe&ab_channel=StatQuestwithJoshStarmer)\n",
    "\n",
    "Scroll down to the bottom for more resources similar to this lecture!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "acddb2cf2bfa22fe258b0acfa21ff626a2f5af6cabd7b3bbd54a8c26bee74faa"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}